Make weight precision smaller to be able to fit memory of small devices
e.g. fp32 to int8

Visual guide to quantization
https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization

https://www.linkedin.com/posts/omarsanseviero_do-you-want-to-learn-about-quantization-ugcPost-7225890124373364736-nywO?utm_source=share&utm_medium=member_android

Making neural nets smaller  
[https://www.linkedin.com/posts/skalskip92_courses-deeplearning-neuralnetwork-activity-7114883448581820416-W6_u?utm_source=share&utm_medium=member_android](https://www.linkedin.com/posts/skalskip92_courses-deeplearning-neuralnetwork-activity-7114883448581820416-W6_u?utm_source=share&utm_medium=member_android)